{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import several essential libraries for data manipulation, preprocessing, and model building"
      ],
      "metadata": {
        "id": "DpV6Yn0GX3qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2oGL7kKMQJeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive in a Google Colab environment\n",
        "\n"
      ],
      "metadata": {
        "id": "TjJJ4rWMYJNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj0bD_WLRrfd",
        "outputId": "f6591da6-bdf8-446e-fbd8-07730b11c23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load training and testing data from TSV (tab-separated values) files"
      ],
      "metadata": {
        "id": "6vrCkZTHYoHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = pd.read_csv('/content/drive/My Drive/Projects/Sentiment Analysis on Movie Reviews- Classify the sentiment of sentences from the Rotten Tomatoes dataset/train.tsv', delimiter='\\t')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/Projects/Sentiment Analysis on Movie Reviews- Classify the sentiment of sentences from the Rotten Tomatoes dataset/test.tsv', delimiter='\\t')\n",
        "\n",
        "# Extract phrases and labels\n",
        "phrases = train_data['Phrase'].values\n",
        "labels = train_data['Sentiment'].values"
      ],
      "metadata": {
        "id": "XD2wOPwCRth4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set hyperparameters"
      ],
      "metadata": {
        "id": "-gMx65RbYw1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "embedding_dim = 16\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<oov>'"
      ],
      "metadata": {
        "id": "T90YHw2tS0H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model and Prediction"
      ],
      "metadata": {
        "id": "pYqfQGxCZfpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_RCvq5mMmBd",
        "outputId": "2af0cd8f-2d7c-41cb-ee3a-415fc12dfe68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 16)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                1088      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,413\n",
            "Trainable params: 161,413\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "3902/3902 - 18s - loss: 1.2164 - accuracy: 0.5200 - val_loss: 1.1678 - val_accuracy: 0.5259 - 18s/epoch - 5ms/step\n",
            "Epoch 2/2\n",
            "3902/3902 - 23s - loss: 1.0595 - accuracy: 0.5783 - val_loss: 0.9806 - val_accuracy: 0.6038 - 23s/epoch - 6ms/step\n",
            "2072/2072 [==============================] - 3s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Tokenize phrases\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(phrases)\n",
        "\n",
        "# Convert phrases to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(phrases)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=2)\n",
        "\n",
        "# Prepare test data\n",
        "test_phrases = test_data['Phrase'].values\n",
        "test_sequences = tokenizer.texts_to_sequences(test_phrases)\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Predict sentiment labels for test data\n",
        "predictions = model.predict(test_padded_sequences)\n",
        "\n",
        "# Convert predictions to sentiment labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "output = pd.DataFrame({'PhraseId': test_data['PhraseId'], 'Sentiment': predicted_labels})\n",
        "output.to_csv('submission.csv', index=False)\n",
        "output.to_csv('/content/drive/My Drive/Projects/Sentiment Analysis on Movie Reviews- Classify the sentiment of sentences from the Rotten Tomatoes dataset/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualize the acuuracy and loss"
      ],
      "metadata": {
        "id": "Pa49eR2Cczgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_' + metric])\n",
        "    plt.legend([metric, 'val_' + metric])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(history, \"accuracy\")\n",
        "plot_metric(history, \"loss\")"
      ],
      "metadata": {
        "id": "DWmCsxYIZHNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}